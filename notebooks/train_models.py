"""
–°–∫—Ä–∏–ø—Ç –æ–±—É—á–µ–Ω–∏—è ML –º–æ–¥–µ–ª–µ–π –¥–ª—è –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ñ–∏—à–∏–Ω–≥–∞
–î–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞: –†–∞–∑—Ä–∞–±–æ—Ç–∫–∞ –º–µ—Ç–æ–¥–æ–≤ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∏—è —Ñ–∏—à–∏–Ω–≥–æ–≤—ã—Ö –∞—Ç–∞–∫ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –ò–ò

–ê–≤—Ç–æ—Ä: Tamiris
–î–∞—Ç–∞: 2025

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç:
1. –ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏ –ø–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ—Ç –¥–∞—Ç–∞—Å–µ—Ç
2. –ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ URL
3. –û–±—É—á–∞–µ—Ç —Ç—Ä–∏ –º–æ–¥–µ–ª–∏: Logistic Regression, Random Forest, XGBoost
4. –û—Ü–µ–Ω–∏–≤–∞–µ—Ç –∏ —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –º–æ–¥–µ–ª–∏
5. –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
"""

import os
import pickle
import warnings
from pathlib import Path
from datetime import datetime

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold
from sklearn.preprocessing import StandardScaler
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import (
    accuracy_score, 
    precision_score, 
    recall_score, 
    f1_score,
    confusion_matrix,
    classification_report,
    roc_auc_score,
    roc_curve
)
import xgboost as xgb

# –ü–æ–¥–∞–≤–ª—è–µ–º –ø—Ä–µ–¥—É–ø—Ä–µ–∂–¥–µ–Ω–∏—è –¥–ª—è —á–∏—Å—Ç–æ–≥–æ –≤—ã–≤–æ–¥–∞
warnings.filterwarnings('ignore')

# –ü—É—Ç—å –¥–ª—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –º–æ–¥–µ–ª–µ–π
MODEL_DIR = Path(__file__).parent.parent / 'app' / 'ml' / 'models'
MODEL_DIR.mkdir(parents=True, exist_ok=True)


def load_real_dataset() -> pd.DataFrame:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ä–µ–∞–ª—å–Ω—ã–π –¥–∞—Ç–∞—Å–µ—Ç —Ñ–∏—à–∏–Ω–≥–æ–≤—ã—Ö URL
    
    –ò—Å—Ç–æ—á–Ω–∏–∫: GregaVrbancic/Phishing-Dataset (GitHub)
    –°–æ–¥–µ—Ä–∂–∏—Ç 88,647 URL —Å 111 –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏
    
    Returns:
        DataFrame —Å –ø—Ä–∏–∑–Ω–∞–∫–∞–º–∏ –∏ –º–µ—Ç–∫–∞–º–∏
    """
    import urllib.request
    import io
    
    print("=" * 60)
    print("–ó–ê–ì–†–£–ó–ö–ê –†–ï–ê–õ–¨–ù–û–ì–û –î–ê–¢–ê–°–ï–¢–ê")
    print("=" * 60)
    
    # URL –¥–∞—Ç–∞—Å–µ—Ç–∞ –Ω–∞ GitHub (dataset_small.csv - 58,645 –æ–±—Ä–∞–∑—Ü–æ–≤)
    DATASET_URL = "https://raw.githubusercontent.com/GregaVrbancic/Phishing-Dataset/master/dataset_small.csv"
    
    print(f"\n–ò—Å—Ç–æ—á–Ω–∏–∫: GregaVrbancic/Phishing-Dataset")
    print(f"URL: {DATASET_URL}")
    print("\n–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞—Ç–∞—Å–µ—Ç–∞...")
    
    try:
        # –ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞—Ç–∞—Å–µ—Ç
        with urllib.request.urlopen(DATASET_URL, timeout=60) as response:
            data = response.read().decode('utf-8')
        
        # –ß–∏—Ç–∞–µ–º –≤ DataFrame
        df = pd.read_csv(io.StringIO(data))
        
        print(f"–î–∞—Ç–∞—Å–µ—Ç –∑–∞–≥—Ä—É–∂–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
        print(f"\n–†–∞–∑–º–µ—Ä –¥–∞—Ç–∞—Å–µ—Ç–∞: {len(df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
        print(f"–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤: {len(df.columns) - 1}")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—É
        if 'phishing' in df.columns:
            # –ü–µ—Ä–µ–∏–º–µ–Ω–æ–≤—ã–≤–∞–µ–º —Ü–µ–ª–µ–≤—É—é –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é
            df = df.rename(columns={'phishing': 'label'})
        elif 'class' in df.columns:
            df = df.rename(columns={'class': 'label'})
        elif 'result' in df.columns:
            df = df.rename(columns={'result': 'label'})
        
        # –£–±–µ–¥–∏–º—Å—è —á—Ç–æ label –±–∏–Ω–∞—Ä–Ω—ã–π (0/1)
        if df['label'].dtype == object:
            df['label'] = df['label'].map({'legitimate': 0, 'phishing': 1, 'good': 0, 'bad': 1})
        
        # –ü—Ä–∏–≤–æ–¥–∏–º –≤—Å–µ –∑–Ω–∞—á–µ–Ω–∏—è –∫ —á–∏—Å–ª–æ–≤—ã–º
        for col in df.columns:
            if col != 'label':
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        # –£–¥–∞–ª—è–µ–º —Å—Ç—Ä–æ–∫–∏ —Å –ø—Ä–æ–ø—É—Å–∫–∞–º–∏
        df = df.dropna()
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –ø–æ –∫–ª–∞—Å—Å–∞–º
        legitimate_count = (df['label'] == 0).sum()
        phishing_count = (df['label'] == 1).sum()
        
        print(f"\n–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –∫–ª–∞—Å—Å–æ–≤:")
        print(f"   –õ–µ–≥–∏—Ç–∏–º–Ω—ã—Ö: {legitimate_count} ({legitimate_count/len(df)*100:.1f}%)")
        print(f"   –§–∏—à–∏–Ω–≥–æ–≤—ã—Ö: {phishing_count} ({phishing_count/len(df)*100:.1f}%)")
        
        # –ò—Å–ø–æ–ª—å–∑—É–µ–º –í–°–ï –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞ (–∫—Ä–æ–º–µ label)
        print(f"\n–ò—Å–ø–æ–ª—å–∑—É–µ–º –≤—Å–µ {len(df.columns) - 1} –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ –∏–∑ –¥–∞—Ç–∞—Å–µ—Ç–∞")
        
        return df
        
    except urllib.error.URLError as e:
        print(f"\n–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏: {e}")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∫–∞–∫ fallback...")
        return generate_synthetic_fallback()
    except Exception as e:
        print(f"\n–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –¥–∞–Ω–Ω—ã—Ö: {e}")
        print("–ò—Å–ø–æ–ª—å–∑—É–µ–º —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∫–∞–∫ fallback...")
        return generate_synthetic_fallback()


def generate_synthetic_fallback(n_samples: int = 10000) -> pd.DataFrame:
    """
    –ì–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç –∫–∞–∫ fallback –µ—Å–ª–∏ –∑–∞–≥—Ä—É–∑–∫–∞ –Ω–µ —É–¥–∞–ª–∞—Å—å
    """
    print("\n–ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞ (fallback)...")
    
    np.random.seed(42)
    
    data = {
        'url_length': np.concatenate([
            np.random.normal(45, 15, n_samples // 2).astype(int),
            np.random.normal(85, 25, n_samples // 2).astype(int),
        ]),
        'domain_length': np.concatenate([
            np.random.normal(12, 4, n_samples // 2).astype(int),
            np.random.normal(22, 8, n_samples // 2).astype(int),
        ]),
        'path_length': np.concatenate([
            np.random.normal(15, 10, n_samples // 2).astype(int),
            np.random.normal(35, 15, n_samples // 2).astype(int),
        ]),
        'has_https': np.concatenate([
            np.random.choice([0, 1], n_samples // 2, p=[0.1, 0.9]),
            np.random.choice([0, 1], n_samples // 2, p=[0.6, 0.4]),
        ]),
        'has_ip_address': np.concatenate([
            np.random.choice([0, 1], n_samples // 2, p=[0.99, 0.01]),
            np.random.choice([0, 1], n_samples // 2, p=[0.8, 0.2]),
        ]),
        'subdomain_count': np.concatenate([
            np.random.poisson(0.5, n_samples // 2),
            np.random.poisson(2, n_samples // 2),
        ]),
        'special_char_count': np.concatenate([
            np.random.poisson(3, n_samples // 2),
            np.random.poisson(8, n_samples // 2),
        ]),
        'suspicious_keyword_count': np.concatenate([
            np.random.poisson(0.2, n_samples // 2),
            np.random.poisson(3, n_samples // 2),
        ]),
        'entropy_score': np.concatenate([
            np.random.normal(3.5, 0.5, n_samples // 2),
            np.random.normal(4.5, 0.7, n_samples // 2),
        ]),
    }
    
    labels = np.concatenate([np.zeros(n_samples // 2), np.ones(n_samples // 2)])
    
    df = pd.DataFrame(data)
    df['label'] = labels
    df = df.sample(frac=1, random_state=42).reset_index(drop=True)
    
    print(f"–°–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –¥–∞—Ç–∞—Å–µ—Ç —Å–æ–∑–¥–∞–Ω: {len(df)} –æ–±—Ä–∞–∑—Ü–æ–≤")
    
    return df


def train_and_evaluate_models(df: pd.DataFrame):
    """
    –û–±—É—á–∞–µ—Ç –∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç ML –º–æ–¥–µ–ª–∏
    """
    print("\n" + "="*60)
    print("–û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –ø—Ä–∏–∑–Ω–∞–∫–∏ –∏ –º–µ—Ç–∫–∏
    X = df.drop('label', axis=1)
    y = df['label']
    
    feature_names = X.columns.tolist()
    
    # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/test
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.2, random_state=42, stratify=y
    )
    
    print(f"\nüìà –†–∞–∑–º–µ—Ä –æ–±—É—á–∞—é—â–µ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_train)}")
    print(f"üìä –†–∞–∑–º–µ—Ä —Ç–µ—Å—Ç–æ–≤–æ–π –≤—ã–±–æ—Ä–∫–∏: {len(X_test)}")
    
    # –ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –ø—Ä–∏–∑–Ω–∞–∫–æ–≤
    scaler = StandardScaler()
    X_train_scaled = scaler.fit_transform(X_train)
    X_test_scaled = scaler.transform(X_test)
    
    # –û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    models = {
        'logistic_regression': LogisticRegression(
            max_iter=1000,
            random_state=42,
            C=1.0,
            solver='lbfgs'
        ),
        'random_forest': RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            min_samples_split=5,
            random_state=42,
            n_jobs=-1
        ),
        'xgboost': xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='logloss',
            use_label_encoder=False
        ),
    }
    
    results = {}
    
    for name, model in models.items():
        print(f"\n{'‚îÄ'*40}")
        print(f"üì¶ –û–±—É—á–µ–Ω–∏–µ: {name}")
        print('‚îÄ'*40)
        
        # –û–±—É—á–µ–Ω–∏–µ
        if name == 'logistic_regression':
            model.fit(X_train_scaled, y_train)
            y_pred = model.predict(X_test_scaled)
            y_prob = model.predict_proba(X_test_scaled)[:, 1]
        else:
            model.fit(X_train, y_train)
            y_pred = model.predict(X_test)
            y_prob = model.predict_proba(X_test)[:, 1]
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        accuracy = accuracy_score(y_test, y_pred)
        precision = precision_score(y_test, y_pred)
        recall = recall_score(y_test, y_pred)
        f1 = f1_score(y_test, y_pred)
        roc_auc = roc_auc_score(y_test, y_prob)
        
        # –ö—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏—è
        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
        if name == 'logistic_regression':
            cv_scores = cross_val_score(model, X_train_scaled, y_train, cv=cv, scoring='accuracy')
        else:
            cv_scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='accuracy')
        
        results[name] = {
            'model': model,
            'accuracy': accuracy,
            'precision': precision,
            'recall': recall,
            'f1': f1,
            'roc_auc': roc_auc,
            'cv_mean': cv_scores.mean(),
            'cv_std': cv_scores.std(),
        }
        
        print(f"  Accuracy:  {accuracy:.4f}")
        print(f"  Precision: {precision:.4f}")
        print(f"  Recall:    {recall:.4f}")
        print(f"  F1 Score:  {f1:.4f}")
        print(f"  ROC AUC:   {roc_auc:.4f}")
        print(f"  CV Score:  {cv_scores.mean():.4f} (¬±{cv_scores.std():.4f})")
        
        # Confusion Matrix
        cm = confusion_matrix(y_test, y_pred)
        print(f"\n  Confusion Matrix:")
        print(f"  TP: {cm[1,1]:4d}  FP: {cm[0,1]:4d}")
        print(f"  FN: {cm[1,0]:4d}  TN: {cm[0,0]:4d}")
        
        # Feature Importance (–¥–ª—è RF –∏ XGBoost)
        if name in ['random_forest', 'xgboost']:
            importances = model.feature_importances_
            indices = np.argsort(importances)[::-1][:5]
            print(f"\n  Top 5 –≤–∞–∂–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤:")
            for i, idx in enumerate(indices):
                print(f"    {i+1}. {feature_names[idx]}: {importances[idx]:.4f}")
    
    return results, scaler


def save_models(results: dict, scaler):
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç –æ–±—É—á–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
    """
    print("\n" + "="*60)
    print("üíæ –°–û–•–†–ê–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    for name, data in results.items():
        model_path = MODEL_DIR / f"{name}.pkl"
        with open(model_path, 'wb') as f:
            pickle.dump(data['model'], f)
        print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {model_path}")
    
    # –°–æ—Ö—Ä–∞–Ω—è–µ–º scaler
    scaler_path = MODEL_DIR / "scaler.pkl"
    with open(scaler_path, 'wb') as f:
        pickle.dump(scaler, f)
    print(f"‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ: {scaler_path}")


def print_summary(results: dict):
    """
    –í—ã–≤–æ–¥–∏—Ç –∏—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    """
    print("\n" + "="*60)
    print("üìä –ò–¢–û–ì–û–í–û–ï –°–†–ê–í–ù–ï–ù–ò–ï –ú–û–î–ï–õ–ï–ô")
    print("="*60)
    
    print("\n{:<25} {:>10} {:>10} {:>10} {:>10}".format(
        "–ú–æ–¥–µ–ª—å", "Accuracy", "Precision", "Recall", "F1"
    ))
    print("-"*60)
    
    best_model = None
    best_f1 = 0
    
    for name, data in results.items():
        print("{:<25} {:>10.4f} {:>10.4f} {:>10.4f} {:>10.4f}".format(
            name, data['accuracy'], data['precision'], data['recall'], data['f1']
        ))
        if data['f1'] > best_f1:
            best_f1 = data['f1']
            best_model = name
    
    print("-"*60)
    print(f"\nüèÜ –õ—É—á—à–∞—è –º–æ–¥–µ–ª—å: {best_model} (F1: {best_f1:.4f})")
    
    return best_model


def main():
    """
    –ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è –æ–±—É—á–µ–Ω–∏—è
    """
    print("\n" + "="*60)
    print("üõ°Ô∏è  PHISHGUARD - –û–ë–£–ß–ï–ù–ò–ï ML –ú–û–î–ï–õ–ï–ô")
    print("    –î–∏–ø–ª–æ–º–Ω–∞—è —Ä–∞–±–æ—Ç–∞: –û–±–Ω–∞—Ä—É–∂–µ–Ω–∏–µ —Ñ–∏—à–∏–Ω–≥–∞ —Å –ò–ò")
    print("="*60)
    print(f"    –î–∞—Ç–∞: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    print("="*60)
    
    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∞–ª—å–Ω–æ–≥–æ –¥–∞—Ç–∞—Å–µ—Ç–∞
    df = load_real_dataset()
    
    # 2. –û–±—É—á–µ–Ω–∏–µ –∏ –æ—Ü–µ–Ω–∫–∞ –º–æ–¥–µ–ª–µ–π
    results, scaler = train_and_evaluate_models(df)
    
    # 3. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ –º–æ–¥–µ–ª–µ–π
    save_models(results, scaler)
    
    # 4. –ò—Ç–æ–≥–æ–≤–æ–µ —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
    best_model = print_summary(results)
    
    print("\n" + "="*60)
    print("‚úÖ –û–ë–£–ß–ï–ù–ò–ï –ó–ê–í–ï–†–®–ï–ù–û!")
    print("="*60)
    print(f"\nüìÅ –ú–æ–¥–µ–ª–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {MODEL_DIR}")
    print("\nüöÄ –î–ª—è –∑–∞–ø—É—Å–∫–∞ API –≤—ã–ø–æ–ª–Ω–∏—Ç–µ:")
    print("   cd phishing-detector-api")
    print("   uvicorn app.main:app --reload")
    print()


if __name__ == "__main__":
    main()
